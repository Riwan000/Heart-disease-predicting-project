{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the tools required\n",
    "\n",
    "# Regular EDA (Exploratory Data Analysis) and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline \n",
    "# We want to view the plots in the IDE\n",
    "\n",
    "# Models from Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for fitting and scoring the models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Fits and trains the model then provide a score usind the test data set.\n",
    "    About the parameters :-\n",
    "    * models : Provide the variable containing the models.\n",
    "    * X_train : Training data without the labels\n",
    "    * X_test : Test data without the labels\n",
    "    * y_train : Training labels\n",
    "    * y_test : Test labels \n",
    "\n",
    "    '''\n",
    "\n",
    "    # Set a random seed \n",
    "    np.random.seed(45)\n",
    "\n",
    "    # Model score dictionary to append the scores \n",
    "    model_score = {}\n",
    "\n",
    "    # Looping through models\n",
    "    for name, model in models.items():\n",
    "        # Fitting the model  \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Appending the score of the model into the dictionary\n",
    "        model_score[name] = model.score(X_test, y_test)        \n",
    "\n",
    "    return model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the confusion matrix using seaborne\n",
    "sns.set_theme(font_scale=1.5) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True label\") # true labels go on the y-axis \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_eval_metric(name, model) :\n",
    "\n",
    "    model_name = str(name)\n",
    "\n",
    "    accuracy = cross_val_score(model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         cv= 5,\n",
    "                         scoring= 'accuracy')\n",
    "    accuracy = np.mean(accuracy)\n",
    "\n",
    "    precision = cross_val_score(model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         cv= 5,\n",
    "                         scoring= 'precision')\n",
    "    precision = np.mean(precision)\n",
    "\n",
    "    recall = cross_val_score(model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         cv= 5,\n",
    "                         scoring= 'recall')\n",
    "    recall = np.mean(recall)\n",
    "\n",
    "    f1_score = cross_val_score(model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         cv= 5,\n",
    "                         scoring= 'f1')\n",
    "\n",
    "    f1_score = np.mean(f1_score)\n",
    "\n",
    "\n",
    "    metric_name = ['Accuracy', 'Precision', 'Recall', 'F1 score']\n",
    "    metrics_var = [accuracy, precision, recall, f1_score]\n",
    "    metrics = dict(zip(metric_name, metrics_var))\n",
    "\n",
    "    \n",
    "    return model_name, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
